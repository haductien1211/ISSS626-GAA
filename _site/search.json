[
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html",
    "title": "Take-home Exercise 2",
    "section": "",
    "text": "Tourism is one of Thailand’s largest industries, accounting for some 20% of the gross domestic product (GDP). In 2019, Thailand earned 90 billion USD from domestic and international tourism, but the COVID-19 pandemic caused revenues to crash to 24 billion USD in 2020. However, it is important to note that the tourism economy of Thailand are not evenly distributed.\nWe are interested to discover:\n\nIf the key indicators of tourism economy of Thailand are independent from space and space and time.\nIf the tourism economy is indeed spatial and spatio-temporal dependent, then, we would like to detect where are the clusters and outliers, and the emerging hot spot/cold spot areas.\n\n\n\n\nThe specific tasks of this take-home exercise are as follows:\n\nUsing appropriate function of sf and tidyverse, preparing the following geospatial data layer:\n\na study area layer in sf polygon features. It must be at province level (including Bangkok) of Thailand.\na tourism economy indicators layer within the study area in sf polygon features.\na derived tourism economy indicator layer in spacetime s3 class of sfdep. Keep the time series at month and year levels.\n\nUsing the extracted data, perform global spatial autocorrelation analysis by using sfdep methods.\nUsing the extracted data, perform local spatial autocorrelation analysis by using sfdep methods.\nUsing the extracted data, perform emerging hotspot analysis by using sfdep methods.\nDescribe the spatial patterns revealed by the analysis above.\n\n\n\n\n\npacman::p_load(sf, sfdep, spdep, tmap, plotly, tidyverse, Kendall)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#objectives",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#objectives",
    "title": "Take-home Exercise 2",
    "section": "",
    "text": "Tourism is one of Thailand’s largest industries, accounting for some 20% of the gross domestic product (GDP). In 2019, Thailand earned 90 billion USD from domestic and international tourism, but the COVID-19 pandemic caused revenues to crash to 24 billion USD in 2020. However, it is important to note that the tourism economy of Thailand are not evenly distributed.\nWe are interested to discover:\n\nIf the key indicators of tourism economy of Thailand are independent from space and space and time.\nIf the tourism economy is indeed spatial and spatio-temporal dependent, then, we would like to detect where are the clusters and outliers, and the emerging hot spot/cold spot areas."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#the-tasks",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#the-tasks",
    "title": "Take-home Exercise 2",
    "section": "",
    "text": "The specific tasks of this take-home exercise are as follows:\n\nUsing appropriate function of sf and tidyverse, preparing the following geospatial data layer:\n\na study area layer in sf polygon features. It must be at province level (including Bangkok) of Thailand.\na tourism economy indicators layer within the study area in sf polygon features.\na derived tourism economy indicator layer in spacetime s3 class of sfdep. Keep the time series at month and year levels.\n\nUsing the extracted data, perform global spatial autocorrelation analysis by using sfdep methods.\nUsing the extracted data, perform local spatial autocorrelation analysis by using sfdep methods.\nUsing the extracted data, perform emerging hotspot analysis by using sfdep methods.\nDescribe the spatial patterns revealed by the analysis above."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#the-packages",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#the-packages",
    "title": "Take-home Exercise 2",
    "section": "",
    "text": "pacman::p_load(sf, sfdep, spdep, tmap, plotly, tidyverse, Kendall)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#importing-the-raw-data",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#importing-the-raw-data",
    "title": "Take-home Exercise 2",
    "section": "2.1 Importing the raw data",
    "text": "2.1 Importing the raw data\nFor the purpose of this take-home exercise, two data sets shall be used, they are:\n\nThailand Domestic Tourism Statistics at Kaggle. We are required to use version 2 of the data set.\nThailand - Subnational Administrative Boundaries at HDX. We are required to use the province boundary data set.\n\nThe code chunk below is used to import Thailand - Subnational Administrative Boundaries as well as filtering out the region of study which is the Bangkok Metropolitan Region BMR and converting the projected coordinate system of data to WGS 84 / UTM zone 47N and the EPSG code is 32647 to create THSAB_sf\n\nTHSAB_sf &lt;- st_read(dsn = \"data/geo\", \n                         layer = \"tha_admbnda_adm1_rtsd_20220121\") %&gt;%\n  st_transform(crs = 32647)\n\nReading layer `tha_admbnda_adm1_rtsd_20220121' from data source \n  `C:\\Users\\tien_\\OneDrive\\SMU\\haductien1211\\ISSS626-GAA\\Take-home_Ex\\Take-home_Ex02\\data\\geo' \n  using driver `ESRI Shapefile'\nSimple feature collection with 77 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97.34336 ymin: 5.613038 xmax: 105.637 ymax: 20.46507\nGeodetic CRS:  WGS 84\n\n\nThis code chunk is to import Thailand Domestic Tourism Statistics data and create tourism.\nIn the below code I will also create 2 new columns for the Month and Year separately for the purpose of using them for later analysis as well as converting the province_thai column name to ADM1_TH for the purpose of left-joining with the GEO data later and removing the date column as the 2 new Month and Year column are already created.\n\ntourism &lt;- read_csv(\"data/non-geo/thailand_domestic_tourism_2019_2023_ver2.csv\") %&gt;%\n  mutate(`month` = as.numeric(format(as.Date(`date`), \"%m\"))) %&gt;%\n  mutate(`year` = as.numeric(format(as.Date(`date`), \"%Y\"))) %&gt;%\n  rename(`ADM1_TH` = `province_thai`) %&gt;%\n  select(2:9)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#visualising-foreign-revenue-indicator",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#visualising-foreign-revenue-indicator",
    "title": "Take-home Exercise 2",
    "section": "3.1 Visualising Foreign Revenue Indicator",
    "text": "3.1 Visualising Foreign Revenue Indicator\nFirst I want to merge the yearly foreign revenue table revenue_foreign_year with the GEO data THSAB_sf for easier analysis later. This is done using the code below\n\nrevenue_foreign &lt;- revenue_foreign_year %&gt;%\n  left_join(THSAB_sf) %&gt;%\n  select(1:2,4, 7, 20)\n\nBefore we start the analysis let create a spactime data revenue_foreign_st using revenue_foreign for the purpose of study later\n\nrevenue_foreign_st &lt;- spacetime(revenue_foreign,\n                                THSAB_sf,\n                                .loc_col = \"ADM1_EN\",\n                                .time_col = \"year\")\n\nFor the basic visualization I would still want to see if there are any potential cluster and I want to see the changes of Foreign Revenue cluster over the year. Hence for this purpose I would plot 4 graph using the data extract from revenue_foreign_2019. Therefore I will be using bclust style which is a good combination between kmeans and hclust to fill the data\n\nrevenue_foreign_2019 &lt;- revenue_foreign %&gt;%\n  filter(year == 2019)\nrevenue_foreign_2020 &lt;- revenue_foreign %&gt;%\n  filter(year == 2020)\nrevenue_foreign_2021 &lt;- revenue_foreign %&gt;%\n  filter(year == 2021)\nrevenue_foreign_2022 &lt;- revenue_foreign %&gt;%\n  filter(year == 2022)\n\nRF2019 &lt;- tm_shape(st_as_sf(revenue_foreign_2019)) +\n  tm_fill(\"sum_rev\",\n          n = 5,\n          palette=\"Greens\",\n          style = \"bclust\",\n          title = \"Foreign Revenue 2019\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Distribution of Foreign Revenue 2019\")+\n  tm_text(\"ADM1_EN\", size=0.4)\n\nRF2020 &lt;- tm_shape(st_as_sf(revenue_foreign_2020)) +\n  tm_fill(\"sum_rev\",\n          n = 5,\n          palette=\"Greens\",\n          style = \"bclust\",\n          title = \"Foreign Revenue 2020\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Distribution of Foreign Revenue 2020\")+\n  tm_text(\"ADM1_EN\", size=0.4)\n\nRF2021 &lt;- tm_shape(st_as_sf(revenue_foreign_2021)) +\n  tm_fill(\"sum_rev\",\n          n = 5,\n          palette=\"Greens\",\n          style = \"bclust\",\n          title = \"Foreign Revenue 2021\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Distribution of Foreign Revenue 2021\")+\n  tm_text(\"ADM1_EN\", size=0.4)\n\nRF2022 &lt;- tm_shape(st_as_sf(revenue_foreign_2022)) +\n  tm_fill(\"sum_rev\",\n          n = 5,\n          palette=\"Greens\",\n          style = \"bclust\",\n          title = \"Foreign Revenue 2022\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Distribution of Foreign Revenue 2022\")+\n  tm_text(\"ADM1_EN\", size=0.4)\n\n\ntmap_arrange(RF2019, RF2020, RF2021, RF2022, asp=1, ncol=4)\n\n\n\n\n\n\n\n\nCommittee Member: 1(1) 2(1) 3(1) 4(1) 5(1) 6(1) 7(1) 8(1) 9(1) 10(1)\nComputing Hierarchical Clustering\nCommittee Member: 1(1) 2(1) 3(1) 4(1) 5(1) 6(1) 7(1) 8(1) 9(1) 10(1)\nComputing Hierarchical Clustering\nCommittee Member: 1(1) 2(1) 3(1) 4(1) 5(1) 6(1) 7(1) 8(1) 9(1) 10(1)\nComputing Hierarchical Clustering\nCommittee Member: 1(1) 2(1) 3(1) 4(1) 5(1) 6(1) 7(1) 8(1) 9(1) 10(1)\nComputing Hierarchical Clustering"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#global-measures-of-spatial-autocorrelation",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#global-measures-of-spatial-autocorrelation",
    "title": "Take-home Exercise 2",
    "section": "3.2 Global Measures of Spatial Autocorrelation",
    "text": "3.2 Global Measures of Spatial Autocorrelation\nI’ve previously created the Queen contiguity weight matrix thai_wm_q with snap = 400. Next we need to create Row-standardised weights matrix using the code below\n\nthai_rswm_q &lt;- nb2listw(thai_wm_q,\n                        style=\"W\",\n                        zero.policy = TRUE)\nthai_rswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 77 \nNumber of nonzero links: 354 \nPercentage nonzero weights: 5.970653 \nAverage number of links: 4.597403 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1      S2\nW 77 5929 77 37.54724 320.752\n\n\n\n3.2.1 Computing Global Carlo Moran’s I\nThe code chunk below performs Moran’s I statistical testing using moran.test() of spdep. from 2019 to 2022\n\nwm_q_2019 &lt;- revenue_foreign_2019 %&gt;%\n  # select(3:5) %&gt;%\n  mutate(nb = st_contiguity(geometry, snap = 400),\n         wt = st_weights(nb,\n                         style = \"W\",\n                         allow_zero = TRUE),\n         .before = 1) \n\n\nwm_q_2020 &lt;- revenue_foreign_2020 %&gt;%\n  # select(3:5) %&gt;%\n  mutate(nb = st_contiguity(geometry, snap = 400),\n         wt = st_weights(nb,\n                         style = \"W\",\n                         allow_zero = TRUE),\n         .before = 1) \n\n\nwm_q_2021 &lt;- revenue_foreign_2021 %&gt;%\n  # select(3:5) %&gt;%\n  mutate(nb = st_contiguity(geometry, snap = 400),\n         wt = st_weights(nb,\n                         style = \"W\",\n                         allow_zero = TRUE),\n         .before = 1) \n\n\nwm_q_2022 &lt;- revenue_foreign_2022 %&gt;%\n  # select(3:5) %&gt;%\n  mutate(nb = st_contiguity(geometry, snap = 400),\n         wt = st_weights(nb,\n                         style = \"W\",\n                         allow_zero = TRUE),\n         .before = 1) \n\n2019\n\nglobal_moran_perm(wm_q_2019$sum_rev,\n                  wm_q_2019$nb,\n                  wm_q_2019$wt,\n                  nsim = 999)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 1000 \n\nstatistic = -0.0065291, observed rank = 777, p-value = 0.446\nalternative hypothesis: two.sided\n\n\n2020\n\nglobal_moran_perm(wm_q_2020$sum_rev,\n                  wm_q_2020$nb,\n                  wm_q_2020$wt,\n                  nsim = 999)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.0044982, observed rank = 822, p-value = 0.356\nalternative hypothesis: two.sided\n\n\n2021\n\nglobal_moran_perm(wm_q_2021$sum_rev,\n                  wm_q_2021$nb,\n                  wm_q_2021$wt,\n                  nsim = 999)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.011613, observed rank = 953, p-value = 0.094\nalternative hypothesis: two.sided\n\n\n2022\n\nglobal_moran_perm(wm_q_2022$sum_rev,\n                  wm_q_2022$nb,\n                  wm_q_2022$wt,\n                  nsim = 999)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 1000 \n\nstatistic = -0.020738, observed rank = 682, p-value = 0.636\nalternative hypothesis: two.sided\n\n\nAnother way to do this is using the below test method code chunk since we already have the listw of thai_rswm_q\n\nset.seed(1234)\nbperm_2019 = moran.mc(revenue_foreign_2019$sum_rev,\n                listw=thai_rswm_q, \n                nsim=999,\n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm_2019\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  revenue_foreign_2019$sum_rev \nweights: thai_rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.024516, observed rank = 862, p-value = 0.138\nalternative hypothesis: greater\n\n\n\nset.seed(1234)\nbperm_2020 = moran.mc(revenue_foreign_2020$sum_rev,\n                listw=thai_rswm_q, \n                nsim=999,\n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm_2020\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  revenue_foreign_2020$sum_rev \nweights: thai_rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.027343, observed rank = 879, p-value = 0.121\nalternative hypothesis: greater\n\n\n\nset.seed(1234)\nbperm_2021 = moran.mc(revenue_foreign_2021$sum_rev,\n                listw=thai_rswm_q, \n                nsim=999,\n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm_2021\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  revenue_foreign_2021$sum_rev \nweights: thai_rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = -0.014846, observed rank = 855, p-value = 0.145\nalternative hypothesis: greater\n\n\n\nset.seed(1234)\nbperm_2022 = moran.mc(revenue_foreign_2022$sum_rev,\n                listw=thai_rswm_q, \n                nsim=999,\n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm_2022\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  revenue_foreign_2022$sum_rev \nweights: thai_rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = -0.015294, observed rank = 799, p-value = 0.201\nalternative hypothesis: greater\n\n\n\n\n\n\n\n\nTip\n\n\n\nAll the test over the year from 2019-2022 indicate that p-value &gt; 0.05 hence the null hypothesis are not rejected\n\n\n\n\n3.2.2 Visualising Global Moran’s I\nThe code chunk below is used to plot a histogram of Simulated Moran’s I\n\n2019202020212022\n\n\n\nhist(bperm_2019$res, \n     freq=TRUE, \n     breaks=20, \n     xlab=\"Foreign Revenue 2019 Simulated Moran's I\")\nabline(v=0, \n       col=\"red\") \n\n\n\n\n\n\n\n\n\n\n\nhist(bperm_2020$res, \n     freq=TRUE, \n     breaks=20, \n     xlab=\"Foreign Revenue 2020 Simulated Moran's I\")\nabline(v=0, \n       col=\"red\") \n\n\n\n\n\n\n\n\n\n\n\nhist(bperm_2021$res, \n     freq=TRUE, \n     breaks=20, \n     xlab=\"Foreign Revenue 2021 Simulated Moran's I\")\nabline(v=0, \n       col=\"red\") \n\n\n\n\n\n\n\n\n\n\n\nhist(bperm_2022$res, \n     freq=TRUE, \n     breaks=20, \n     xlab=\"Foreign Revenue 2022 Simulated Moran's I\")\nabline(v=0, \n       col=\"red\") \n\n\n\n\n\n\n\n\n\n\n\n\nMI_corr_2019 &lt;- sp.correlogram(thai_wm_q, \n                               revenue_foreign_2019$sum_rev, \n                               order=6, \n                               method=\"I\", \n                               style=\"W\")\nplot(MI_corr_2019)\n\n\n\n\n\n\n\n\n\nMI_corr_2020 &lt;- sp.correlogram(thai_wm_q, \n                               revenue_foreign_2020$sum_rev, \n                               order=6, \n                               method=\"I\", \n                               style=\"W\")\nplot(MI_corr_2020)\n\n\n\n\n\n\n\n\n\nMI_corr_2021 &lt;- sp.correlogram(thai_wm_q, \n                               revenue_foreign_2021$sum_rev, \n                               order=6, \n                               method=\"I\", \n                               style=\"W\")\nplot(MI_corr_2021)\n\n\n\n\n\n\n\n\n\nMI_corr_2022 &lt;- sp.correlogram(thai_wm_q, \n                               revenue_foreign_2022$sum_rev, \n                               order=6, \n                               method=\"I\", \n                               style=\"W\")\nplot(MI_corr_2022)\n\n\n\n\n\n\n\n\n\n\n3.2.3 Computing local Moran’s I\nUsing the above created wm_q data, we could create the LISA Map and visualizaing the local Moran’s I. The below code is used to create the lisa mapping for Local Moran’s I of Foreign revenue at Province level by using local_moran() of sfdep package.\n\nlisa_2019 &lt;- wm_q_2019 %&gt;% \n  mutate(local_moran = local_moran(sum_rev, \n                                   nb, \n                                   wt, \n                                   nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\nlisa_2020 &lt;- wm_q_2020 %&gt;% \n  mutate(local_moran = local_moran(sum_rev, \n                                   nb, \n                                   wt, \n                                   nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\nlisa_2021 &lt;- wm_q_2021 %&gt;% \n  mutate(local_moran = local_moran(sum_rev, \n                                   nb, \n                                   wt, \n                                   nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\nlisa_2022 &lt;- wm_q_2022 %&gt;% \n  mutate(local_moran = local_moran(sum_rev, \n                                   nb, \n                                   wt, \n                                   nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\n\n3.2.4 Visualising local Moran’s I\nVisualising local Moran’s I and p-value for each year\n\n2019202020212022\n\n\n\ntmap_mode(\"plot\")\n\nmap2019_1&lt;- tm_shape(st_as_sf(lisa_2019)) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(\n    main.title = \"local Moran's I of Foreign Revenue 2019\",\n    main.title.size = 2) +\n  tm_text(\"ADM1_EN\", size=0.5)\n\nmap2019_2 &lt;- tm_shape(st_as_sf(lisa_2019)) +\n  tm_fill(\"p_ii\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n          labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of local Moran's I\",\n            main.title.size = 2)+\n  tm_text(\"ADM1_EN\", size=0.5)\n\n\ntmap_arrange(map2019_1, map2019_2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\nmap2020_1&lt;- tm_shape(st_as_sf(lisa_2020)) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(\n    main.title = \"local Moran's I of Foreign Revenue 2020\",\n    main.title.size = 2) +\n  tm_text(\"ADM1_EN\", size=0.5)\n\nmap2020_2 &lt;- tm_shape(st_as_sf(lisa_2020)) +\n  tm_fill(\"p_ii\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n          labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of local Moran's I\",\n            main.title.size = 2)+\n  tm_text(\"ADM1_EN\", size=0.5)\n\n\ntmap_arrange(map2020_1, map2020_2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\nmap2021_1&lt;- tm_shape(st_as_sf(lisa_2021)) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(\n    main.title = \"local Moran's I of Foreign Revenue 2021\",\n    main.title.size = 2) +\n  tm_text(\"ADM1_EN\", size=0.5)\n\nmap2021_2 &lt;- tm_shape(st_as_sf(lisa_2021)) +\n  tm_fill(\"p_ii\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n          labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of local Moran's I\",\n            main.title.size = 2)+\n  tm_text(\"ADM1_EN\", size=0.5)\n\n\ntmap_arrange(map2021_1, map2021_2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\nmap2022_1&lt;- tm_shape(st_as_sf(lisa_2022)) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(\n    main.title = \"local Moran's I of Foreign Revenue 2022\",\n    main.title.size = 2) +\n  tm_text(\"ADM1_EN\", size=0.5)\n\nmap2022_2 &lt;- tm_shape(st_as_sf(lisa_2022)) +\n  tm_fill(\"p_ii\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n          labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of local Moran's I\",\n            main.title.size = 2)+\n  tm_text(\"ADM1_EN\", size=0.5)\n\n\ntmap_arrange(map2022_1, map2022_2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.2.5 Plotting LISA map\nIn lisa sf data.frame, we can find three fields contain the LISA categories. They are mean, median and pysal. In general, classification in mean will be used as shown in the code chunk below.\n\nlisa_sig_2019 &lt;- lisa_2019  %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\nlisa_map_2019 &lt;- tm_shape(st_as_sf(lisa_2019)) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\n  tm_layout(\n    main.title = \"LISA MAP 2019\",\n    main.title.size = 2)+\n  tm_shape(st_as_sf(lisa_sig_2019)) +\n  tm_fill(\"mean\") + \n  tm_borders(alpha = 0.4)+\n  tm_text(\"ADM1_EN\", size=0.5)\n\nlisa_sig_2020 &lt;- lisa_2020  %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\nlisa_map_2020 &lt;- tm_shape(st_as_sf(lisa_2020)) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\n  tm_layout(\n    main.title = \"LISA MAP 2020\",\n    main.title.size = 2)+\n  tm_shape(st_as_sf(lisa_sig_2020)) +\n  tm_fill(\"mean\") + \n  tm_borders(alpha = 0.4)+\n  tm_text(\"ADM1_EN\", size=0.5)\n\nlisa_sig_2021 &lt;- lisa_2021  %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\nlisa_map_2021 &lt;- tm_shape(st_as_sf(lisa_2021)) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\n  tm_layout(\n    main.title = \"LISA MAP 2021\",\n    main.title.size = 2)+\n  tm_shape(st_as_sf(lisa_sig_2021)) +\n  tm_fill(\"mean\") + \n  tm_borders(alpha = 0.4)+\n  tm_text(\"ADM1_EN\", size=0.5)\n\nlisa_sig_2022 &lt;- lisa_2022  %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\nlisa_map_2022 &lt;- tm_shape(st_as_sf(lisa_2022)) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\n  tm_layout(\n    main.title = \"LISA MAP 2022\",\n    main.title.size = 2)+\n  tm_shape(st_as_sf(lisa_sig_2022)) +\n  tm_fill(\"mean\") + \n  tm_borders(alpha = 0.4)+\n  tm_text(\"ADM1_EN\", size=0.5)\n\n\ntmap_mode(\"plot\")\ntmap_arrange(lisa_map_2019, lisa_map_2020, lisa_map_2021, lisa_map_2022, ncol = 4)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#hot-spot-and-cold-spot-area-analysis-hcsa",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#hot-spot-and-cold-spot-area-analysis-hcsa",
    "title": "Take-home Exercise 2",
    "section": "3.3 Hot Spot and Cold Spot Area Analysis (HCSA)",
    "text": "3.3 Hot Spot and Cold Spot Area Analysis (HCSA)\n\n3.3.1 Computing local Gi* statistics\n\nwm_idw_2019 &lt;- revenue_foreign_2019 %&gt;%\n  mutate(nb = include_self(st_contiguity(geometry, snap = 400)),\n         wts = st_inverse_distance(nb, \n                                   geometry, \n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n\nwm_idw_2020 &lt;- revenue_foreign_2020 %&gt;%\n  mutate(nb = include_self(st_contiguity(geometry, snap = 400)),\n         wts = st_inverse_distance(nb, \n                                   geometry, \n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n\nwm_idw_2021 &lt;- revenue_foreign_2021 %&gt;%\n  mutate(nb = include_self(st_contiguity(geometry, snap = 400)),\n         wts = st_inverse_distance(nb, \n                                   geometry, \n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n\nwm_idw_2022 &lt;- revenue_foreign_2022 %&gt;%\n  mutate(nb = include_self(st_contiguity(geometry, snap = 400)),\n         wts = st_inverse_distance(nb, \n                                   geometry, \n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n\nHCSA_2019 &lt;- wm_idw_2019 %&gt;% \n  mutate(local_Gi = local_gstar_perm(\n    sum_rev, nb, wts, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_Gi)\n\n\nHCSA_2020 &lt;- wm_idw_2020 %&gt;% \n  mutate(local_Gi = local_gstar_perm(\n    sum_rev, nb, wts, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_Gi)\n\n\nHCSA_2021 &lt;- wm_idw_2021 %&gt;% \n  mutate(local_Gi = local_gstar_perm(\n    sum_rev, nb, wts, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_Gi)\n\n\nHCSA_2022 &lt;- wm_idw_2022 %&gt;% \n  mutate(local_Gi = local_gstar_perm(\n    sum_rev, nb, wts, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_Gi)\n\n\nHCSA_sig_2019 &lt;- HCSA_2019  %&gt;%\n  filter(p_sim &lt; 0.05)\n\nHCSA_sig_2020 &lt;- HCSA_2020  %&gt;%\n  filter(p_sim &lt; 0.05)\n\nHCSA_sig_2021 &lt;- HCSA_2021  %&gt;%\n  filter(p_sim &lt; 0.05)\n\nHCSA_sig_2022 &lt;- HCSA_2022  %&gt;%\n  filter(p_sim &lt; 0.05)\n\n\ntmap_mode(\"plot\")\n\nHCSA_map_2019 &lt;- tm_shape(st_as_sf(HCSA_2019)) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) + \n  tm_layout(main.title = \"Hot Spot and Cold Spot Area Analysis 2019\",\n            main.title.size = 2)+\n  tm_shape(st_as_sf(HCSA_sig_2019)) +\n  tm_fill(\"cluster\") + \n  tm_borders(alpha = 0.4) +\n  tm_text(\"ADM1_EN\", size=0.5)\n\nHCSA_map_2020 &lt;- tm_shape(st_as_sf(HCSA_2020)) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) + \n  tm_layout(main.title = \"Hot Spot and Cold Spot Area Analysis 2020\",\n            main.title.size = 2)+\n  tm_shape(st_as_sf(HCSA_sig_2020)) +\n  tm_fill(\"cluster\") + \n  tm_borders(alpha = 0.4) +\n  tm_text(\"ADM1_EN\", size=0.5)\n\nHCSA_map_2021 &lt;- tm_shape(st_as_sf(HCSA_2021)) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) + \n  tm_layout(main.title = \"Hot Spot and Cold Spot Area Analysis 2021\",\n            main.title.size = 2)+\n  tm_shape(st_as_sf(HCSA_sig_2021)) +\n  tm_fill(\"cluster\") + \n  tm_borders(alpha = 0.4) +\n  tm_text(\"ADM1_EN\", size=0.5)\n\nHCSA_map_2022 &lt;- tm_shape(st_as_sf(HCSA_2022)) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) + \n  tm_layout(main.title = \"Hot Spot and Cold Spot Area Analysis 2022\",\n            main.title.size = 2)+\n  tm_shape(st_as_sf(HCSA_sig_2022)) +\n  tm_fill(\"cluster\") + \n  tm_borders(alpha = 0.4) +\n  tm_text(\"ADM1_EN\", size=0.5)\n\ntmap_arrange(HCSA_map_2019, HCSA_map_2020, HCSA_map_2021, HCSA_map_2022, ncol = 4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFigure above reveals the changes of hotspot and cold spot over the year."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#emerging-hotspot-analysis",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#emerging-hotspot-analysis",
    "title": "Take-home Exercise 2",
    "section": "3.4 Emerging Hotspot Analysis",
    "text": "3.4 Emerging Hotspot Analysis\nWe previously already create Spacetime revenue_foreign_st that included the foreign revenue data from 2019-2022\n\nis_spacetime_cube(revenue_foreign_st)\n\n[1] TRUE\n\n\n\n3.4.1 Computing Gi*\nDeriving the spatial weights\n\nrevenue_foreign_nb &lt;- revenue_foreign_st %&gt;%\n  activate(\"geometry\") %&gt;%\n  mutate(nb = include_self(st_contiguity(geometry, snap = 400)),\n         wt = st_inverse_distance(nb,\n                                  geometry,\n                                  scale = 1,\n                                  alpha = 1),\n  .before = 1) %&gt;%\n  set_nbs(\"nb\") %&gt;%\n  set_wts(\"wt\")\n\nWe can use these new columns to manually calculate the local Gi* for each location. We can do this by grouping by Year and using local_gstar_perm() of sfdep package. After which, we use unnest() to unnest gi_star column of the newly created gi_starts data.frame.\n\ngi_stars &lt;- revenue_foreign_nb %&gt;%\n  group_by(year) %&gt;%\n  mutate(gi_star = local_gstar_perm(sum_rev,\n                                    nb,\n                                    wt)) %&gt;%\n  unnest(gi_star)\n\nMann-Kendall test data.frame We can replicate this for each location by using group_by() of dplyr package.\n\nehsa &lt;- gi_stars %&gt;%\n  group_by(ADM1_EN) %&gt;%\n  summarise(mk = list(unclass(\n      Kendall::MannKendall(gi_star)\n    )\n  )) %&gt;%\n  unnest_wider(mk)\n\n\n\n3.4.2 Mann-Kendall Test on Gi*\nWith these Gi* measures we can then evaluate each location for a trend using the Mann-Kendall test. The code chunk below uses Bankok county.\n\ncbg &lt;- gi_stars %&gt;% \n  ungroup() %&gt;% \n  filter(ADM1_EN == \"Bangkok\") %&gt;%\n  select(ADM1_EN, year, gi_star)\n\nInteractive Mann-Kendall Plot\n\nggplotly(ggplot(data = cbg, \n       aes(x = year, \n           y = gi_star)) +\n  geom_line() +\n  theme_light())\n\n\n\n\n\n\n\n3.4.3 Performing Emerging Hotspot Analysis\nUsing ehsa We can also sort to show significant emerging hot/cold spots\n\nemerging &lt;- ehsa %&gt;% \n  arrange(sl, abs(tau)) %&gt;% \n  slice(1:10)\nhead(emerging)\n\n# A tibble: 6 × 6\n  ADM1_EN          tau    sl     S     D  varS\n  &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Bangkok       -0.667 0.308    -4  6.00  8.67\n2 Chachoengsao  -0.667 0.308    -4  6.00  8.67\n3 Chanthaburi   -0.667 0.308    -4  6.00  8.67\n4 Chon Buri     -0.667 0.308    -4  6.00  8.67\n5 Lampang        0.667 0.308     4  6.00  8.67\n6 Nakhon Pathom -0.667 0.308    -4  6.00  8.67\n\n\n\nset.seed(1234)\nrevenue_ehsa &lt;- emerging_hotspot_analysis(\n  x = revenue_foreign_nb,\n  .var = \"sum_rev\",\n  k = 1,\n  nsim = 199,\n  nb_col = \"nb\",\n  wt_col = \"wt\"\n)\n\n\nrevenue_foreign_ehsa &lt;- THSAB_sf %&gt;%\n  left_join(revenue_ehsa,\n            by = join_by(ADM1_EN == location))\n\nVisualising the distribution of EHSA classes\n\nggplot(data = revenue_foreign_ehsa,\n       aes(x = classification)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nWe could see majority of location does not has any pattern\n\ntmap_mode('plot')\nrevenue_foreign_ehsa_sig &lt;- revenue_foreign_ehsa  %&gt;%\n  filter(p_value &lt; 1)\n\ntm_shape(st_as_sf(revenue_foreign_ehsa)) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\n  tm_text(\"ADM1_EN\", size=0.5) +\n  tm_shape(st_as_sf(revenue_foreign_ehsa_sig)) +\n  tm_fill(\"classification\") +\n  tm_borders(alpha = 0.4) +\n  tm_text(\"ADM1_EN\", size=0.5)"
  }
]