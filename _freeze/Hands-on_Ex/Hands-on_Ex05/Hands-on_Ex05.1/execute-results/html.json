{
  "hash": "b007e194f63e707f060e642dc26bebf9",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Hands-on Exercise 5.1 Global Measures of Spatial Autocorrelation\"\nauthor: \"Ha Duc Tien\"\ndate: \"September 16, 2024\"\ndate-modified: \"last-modified\"\nexecute: \n  eval: true\n  echo: true\n  warning: false\n  message: false\n  freeze: true\n---\n\n\n# 1. Overview\n\nIn this hands-on exercise, we will learn how to compute Global Measures of Spatial Autocorrelation (GMSA) by using `spdep` package. By the end to this hands-on exercise, we will be able to:\n\n-   import geospatial data using appropriate function(s) of `sf` package,\n-   import csv file using appropriate function of `readr` package,\n-   perform relational join using appropriate join function of `dplyr` package,\n-   compute Global Spatial Autocorrelation (GSA) statistics by using appropriate functions of `spdep` package.\n    -   plot Moran scatterplot.\n    -   compute and plot spatial correlogram using appropriate function of `spdep` package.\n-   provide statistically correct interpretation of GSA statistics.\n\n# 2. Getting Started\n\n## 2.1 The analytical question\n\nIn spatial policy, one of the main development objective of the local government and planners is to ensure equal distribution of development in the province. Our task in this study, hence, is to apply appropriate spatial statistical methods to discover if development are even distributed geographically. If the answer is **No**. Then, our next question will be “is there sign of spatial clustering?”. And, if the answer for this question is yes, then our next question will be “where are these clusters?”\n\nIn this case study, we are interested to examine the spatial pattern of a selected development indicator (i.e. GDP per capita) of [Hunan Provice](https://en.wikipedia.org/wiki/Hunan), People Republic of China.\n\n## 2.2 The Study Area and Data\n\nTwo data sets will be used in this hands-on exercise, they are:\n\n-   Hunan province administrative boundary layer at county level. This is a geospatial data set in ESRI shapefile format.\n-   Hunan_2012.csv: This csv file contains selected Hunan’s local development indicators in 2012.\n\n## 2.3 Setting the Analytical Tools\n\nBefore we get started, we need to ensure that `spdep`, `sf`, `tmap` and `tidyverse` packages of R are currently installed in your R.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(sf, spdep, tmap, tidyverse)\n```\n:::\n\n\n# 3. Getting the Data Into R Environment\n\nIn this section, we will learn how to bring a geospatial data and its associated attribute table into R environment. The geospatial data is in ESRI shapefile format and the attribute table is in csv fomat.\n\n## 3.1 Import shapefile into r environment\n\nThe code chunk below uses [st_read()](https://r-spatial.github.io/sf/reference/st_read.html) of `sf` package to import Hunan shapefile into R. The imported shapefile will be simple features Object of sf.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhunan <- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nReading layer `Hunan' from data source \n  `C:\\Users\\tien_\\OneDrive\\SMU\\haductien1211\\ISSS626-GAA\\Hands-on_Ex\\Hands-on_Ex05\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n```\n\n\n:::\n:::\n\n\n## 3.2 Import csv file into r environment\n\nNext, we will import Hunan_2012.csv into R by using [read_csv()](https://readr.tidyverse.org/reference/read_delim.html) of `readr` package. The output is R data frame class.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhunan2012 <- read_csv(\"data/aspatial/Hunan_2012.csv\")\n```\n:::\n\n\n## 3.3 Performing relational join\n\nThe code chunk below will be used to update the attribute table of hunan’s SpatialPolygonsDataFrame with the attribute fields of hunan2012 dataframe. This is performed by using [left_join()](https://dplyr.tidyverse.org/reference/mutate-joins.html) of `dplyr` package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhunan <- left_join(hunan,hunan2012) %>%\n  select(1:4, 7, 15)\n```\n:::\n\n\n## 3.4 Visualising Regional Development Indicator\n\nNow, we are going to prepare a basemap and a choropleth map showing the distribution of GDPPC 2012 by using `qtm()` of `tmap` package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nequal <- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal interval classification\")\n\nquantile <- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal quantile classification\")\n\ntmap_arrange(equal, \n             quantile, \n             asp=1, \n             ncol=2)\n```\n\n::: {.cell-output-display}\n![](Hands-on_Ex05.1_files/figure-html/unnamed-chunk-5-1.png){width=1200}\n:::\n:::\n\n\n# 4. Global Measures of Spatial Autocorrelation\n\nIn this section, you will learn how to compute global spatial autocorrelation statistics and to perform spatial complete randomness test for global spatial autocorrelation.\n\n## 4.1 Computing Contiguity Spatial Weights\n\nBefore we can compute the global spatial autocorrelation statistics, we need to construct a spatial weights of the study area. The spatial weights is used to define the neighbourhood relationships between the geographical units (i.e. county) in the study area.\n\nIn the code chunk below, [poly2nb()](https://r-spatial.github.io/spdep/reference/poly2nb.html) of `spdep` package is used to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries. If you look at the documentation you will see that you can pass a “queen” argument that takes TRUE or FALSE as options. If you do not specify this argument the default is set to TRUE, that is, if you don’t specify queen = FALSE this function will return a list of first order neighbours using the Queen criteria.\n\nMore specifically, the code chunk below is used to compute Queen contiguity weight matrix.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwm_q <- poly2nb(hunan, \n                queen=TRUE)\nsummary(wm_q)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n```\n\n\n:::\n:::\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one neighbours.\n\n## 4.2 Row-standardised weights matrix\n\nNext, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style=“W”). This is accomplished by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values. While this is the most intuitive way to summaries the neighbors’ values it has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons thus potentially over- or under-estimating the true nature of the spatial autocorrelation in the data. For this example, we’ll stick with the style=“W” option for simplicity’s sake but note that other more robust options are available, notably style=“B”.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrswm_q <- nb2listw(wm_q, \n                   style=\"W\", \n                   zero.policy = TRUE)\nrswm_q\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n```\n\n\n:::\n:::\n\n\n::: callout-note\n## What can we learn from the code chunk above?\n\n-   The input of [`nb2listw()`](https://r-spatial.github.io/spdep/reference/nb2listw.html) must be an object of class nb. The syntax of the function has two major arguments, namely style and zero.poly.\n-   `style` can take values “W”, “B”, “C”, “U”, “minmax” and “S”. B is the basic binary coding, W is row standardised (sums over all links to n), C is globally standardised (sums over all links to n), U is equal to C divided by the number of neighbours (sums over all links to unity), while S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. 1999, p. 167-168 (sums over all links to n).\n-   If `zero.policy` is set to TRUE, weights vectors of zero length are inserted for regions without neighbour in the neighbours list. These will in turn generate lag values of zero, equivalent to the sum of products of the zero row t(rep(0, length=length(neighbours))) %\\*% x, for arbitrary numerical vector x of length length(neighbours). The spatially lagged value of x for the zero-neighbour region will then be zero, which may (or may not) be a sensible choice.\n:::\n\n# 5. Global Measures of Spatial Autocorrelation: Moran’s I\n\nIn this section, you will learn how to perform Moran’s I statistics testing by using [moran.test()](https://r-spatial.github.io/spdep/reference/moran.test.html) of `spdep`.\n\n## 5.1 Maron’s I test\n\nThe code chunk below performs Moran’s I statistical testing using `moran.test()` of `spdep`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmoran.test(hunan$GDPPC, \n           listw=rswm_q, \n           zero.policy = TRUE, \n           na.action=na.omit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tMoran I test under randomisation\n\ndata:  hunan$GDPPC  \nweights: rswm_q    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n```\n\n\n:::\n:::\n\n\n\n**Question: What statistical conclusion can you draw from the output above?**\n\np-value < 0.05, null hypothesis is rejected and we conclude that the spatial processes promoting the observed pattern of values is not random.\n\n",
    "supporting": [
      "Hands-on_Ex05.1_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}